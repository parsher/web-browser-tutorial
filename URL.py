import socket
import ssl
import gzip
import zlib
import brotli
import os
import base64
import time
from urllib.parse import urlparse, unquote, unquote_to_bytes, urljoin
from pathlib import Path
import atexit


class URL:
    """URLì„ íŒŒì‹±í•˜ê³  ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    # í´ë˜ìŠ¤ ë³€ìˆ˜: ì†Œì¼“ ìºì‹œ (host:portë¥¼ í‚¤ë¡œ ì‚¬ìš©)
    _socket_cache = {}
    
    # í´ë˜ìŠ¤ ë³€ìˆ˜: ì½˜í…ì¸  ìºì‹œ {url: {body, headers, timestamp, max_age}}
    _content_cache = {}
    
    # ìºì‹œ ê°€ëŠ¥í•œ íŒŒì¼ í™•ì¥ì
    _CACHEABLE_EXTENSIONS = {
        '.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg', '.ico',  # ì´ë¯¸ì§€
        '.css',  # ìŠ¤íƒ€ì¼
        '.js', '.mjs',  # ìŠ¤í¬ë¦½íŠ¸
        '.woff', '.woff2', '.ttf', '.eot',  # í°íŠ¸
    }
    
    @staticmethod
    def _is_cacheable(url_path):
        """ìºì‹œ ê°€ëŠ¥í•œ ë¦¬ì†ŒìŠ¤ì¸ì§€ í™•ì¸"""
        ext = os.path.splitext(url_path.lower())[1]
        return ext in URL._CACHEABLE_EXTENSIONS
    
    @staticmethod
    def _parse_cache_control(cache_control_header):
        """
        Cache-Control í—¤ë” íŒŒì‹±
        ë¦¬í„´: (no_store: bool, max_age: int or None)
        """
        if not cache_control_header:
            return False, None
        
        directives = [d.strip().lower() for d in cache_control_header.split(',')]
        no_store = False
        max_age = None
        
        for directive in directives:
            if directive == 'no-store':
                no_store = True
            elif directive.startswith('max-age='):
                try:
                    max_age = int(directive.split('=')[1])
                except (ValueError, IndexError):
                    pass
        
        return no_store, max_age
    
    @staticmethod
    def _get_from_cache(full_url):
        """ìºì‹œì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë§Œë£Œ í™•ì¸)"""
        if full_url not in URL._content_cache:
            return None
        
        cache_entry = URL._content_cache[full_url]
        timestamp = cache_entry['timestamp']
        max_age = cache_entry['max_age']
        
        # max_ageê°€ ì—†ìœ¼ë©´ ì˜êµ¬ ìºì‹œ
        if max_age is None:
            return cache_entry
        
        # max_age í™•ì¸
        elapsed = time.time() - timestamp
        if elapsed < max_age:
            return cache_entry
        else:
            # ë§Œë£Œë¨ - ìºì‹œì—ì„œ ì œê±°
            print(f"â° ìºì‹œ ë§Œë£Œ: {full_url}")
            del URL._content_cache[full_url]
            return None
    
    @staticmethod
    def _save_to_cache(full_url, body, headers, max_age):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        URL._content_cache[full_url] = {
            'body': body,
            'headers': headers,
            'timestamp': time.time(),
            'max_age': max_age
        }
        print(f"ğŸ’¾ ìºì‹œ ì €ì¥: {full_url} (max-age: {max_age if max_age else 'unlimited'})")
    
    def __init__(self, url):
        # ë” ì•ˆì •ì ì¸ íŒŒì‹±ì„ ìœ„í•´ urllib.parse ì‚¬ìš©
        parsed = urlparse(url)
        self.scheme = parsed.scheme

        if self.scheme in ["http", "https"]:
            # hostì™€ path ë¶„ë¦¬
            self.host = parsed.netloc
            self.path = parsed.path or "/"
            # í¬íŠ¸ë²ˆí˜¸ ì„¤ì • (httpëŠ” 80, httpsëŠ” 443)
            if self.scheme == "http":
                self.port = 80
            elif self.scheme == "https":
                self.port = 443
        elif self.scheme == "file":
            # file URL: file:///C:/path or file:///home/user/file
            # parsed.netlocëŠ” ë³´í†µ ë¹ˆ ë¬¸ìì—´ ë˜ëŠ” 'localhost'
            # unquoteí•˜ì§€ ì•Šìœ¼ë©´ osê°€ ì‹¤ì œ ê²½ë¡œë¥¼ ì°¾ì§€ë¥¼ ëª»í•¨
            raw_path = unquote(parsed.path)
            # Windows ë“œë¼ì´ë¸Œ í‘œê¸° ì²˜ë¦¬: '/C:/path' -> 'C:/path'
            if os.name == 'nt' and raw_path.startswith("/") and len(raw_path) > 2 and raw_path[2] == ':' :
                raw_path = raw_path.lstrip('/')
            # ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ì €ì¥
            self.filepath = raw_path
        elif self.scheme == "data":
            # data:[<mediatype>][;base64],<data>
            # parsed.path may contain the whole data part; use the original URL
            data_part = url.split(":", 1)[1]
            try:
                meta, data = data_part.split(",", 1)
            except ValueError:
                raise ValueError("Invalid data URI: missing comma separator")
            meta_parts = meta.split(";") if meta else []
            mediatype = meta_parts[0] if meta_parts and meta_parts[0] else "text/plain"
            is_base64 = "base64" in meta_parts
            # extract charset if present
            charset = None
            for part in meta_parts:
                if part.startswith("charset="):
                    charset = part.split("=", 1)[1]
                    break
            # decode data
            if is_base64:
                try:
                    data_bytes = base64.b64decode(data)
                except Exception as e:
                    raise ValueError(f"Invalid base64 data in data URI: {e}")
            else:
                # percent-decoded bytes
                data_bytes = unquote_to_bytes(data)
            # store for request()
            self.data_bytes = data_bytes
            self.data_mediatype = mediatype
            self.data_charset = charset
        elif self.scheme == "view-source":
            # view-source:<inner-uri> -> store inner URL object to fetch its source
            # extract the remainder after the first ':' (preserve // for http/https)
            inner_uri = url[len('view-source:'):]
            # allow whitespace tolerance
            inner_uri = inner_uri.strip()
            # create URL object for inner resource
            self.inner = URL(inner_uri)
        else:
            raise AssertionError(f"Unknown scheme {self.scheme}")
    
    def request(self, redirects: int = 5, redirect_log=None):
        """ì„œë²„ì— HTTP ìš”ì²­ì„ ë³´ë‚´ê³  ì‘ë‹µì„ ë°›ëŠ” í•¨ìˆ˜"""
        # redirect_log ì´ˆê¸°í™” (ìµœìƒìœ„ í˜¸ì¶œìê°€ Noneì„ ì£¼ë©´ ì—¬ê¸°ì„œ ìƒì„±í•˜ê³ 
        # ìµœì¢… ê²°ê³¼ ì§ì „ì— ë¡œê·¸ë¥¼ ì¶œë ¥í•¨)
        created_local_log = False
        if redirect_log is None:
            redirect_log = []
            created_local_log = True

        # view-sourceì¸ ê²½ìš° ë‚´ë¶€ URLì˜ ë³¸ë¬¸ì„ ê°€ì ¸ì™€ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if getattr(self, 'scheme', None) == 'view-source':
            # view-source:example.comì˜ innerëŠ” example.comì„ ë”°ë¼ì„œ ë°˜ë“œì‹œ ìˆì–´ì•¼í•¨
            if not hasattr(self, 'inner'):
                raise ValueError('view-source missing inner URL')
            body = self.inner.request(redirects=redirects, redirect_log=redirect_log)
            return body
        # data ìŠ¤í‚´ì´ë©´ URIì— í¬í•¨ëœ ë°ì´í„°ë¥¼ ë°˜í™˜
        if getattr(self, 'scheme', None) == 'data':
            # Determine charset to decode bytes to text
            charset = self.data_charset or ("utf-8" if self.data_mediatype.startswith("text/") else "utf-8")
            try:
                return self.data_bytes.decode(charset, errors='replace')
            except Exception:
                return self.data_bytes.decode('utf-8', errors='replace')

        # file ìŠ¤í‚´ì´ë©´ ë¡œì»¬ íŒŒì¼ì„ ì½ì–´ì„œ ë‚´ìš©ì„ ë°˜í™˜
        if getattr(self, 'scheme', None) == 'file':
            # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if not os.path.exists(self.filepath):
                raise FileNotFoundError(f"File not found: {self.filepath}")
            with open(self.filepath, 'rb') as f:
                data = f.read()
                # 'ï¿½ï¿½ invalid utf8 ï¿½' ì´ì²˜ëŸ¼ ë³€í™˜ì´ ë¨, U+FFFD
            return data.decode('utf8', errors='replace')
        
        # HTTP/HTTPS ìš”ì²­ì— ëŒ€í•œ ìºì‹œ ì²˜ë¦¬
        full_url = f"{self.scheme}://{self.host}{self.path}"
        
        # ìºì‹œ ê°€ëŠ¥í•œ ë¦¬ì†ŒìŠ¤ì¸ì§€ í™•ì¸
        is_cacheable = URL._is_cacheable(self.path)
        
        # ìºì‹œ í™•ì¸
        if is_cacheable:
            cached = URL._get_from_cache(full_url)
            if cached:
                print(f"âš¡ ìºì‹œì—ì„œ ë°˜í™˜: {full_url}")
                return cached['body']
        
        # 1. ì†Œì¼“ ìºì‹œ í™•ì¸ ë° ì¬ì‚¬ìš©
        cache_key = f"{self.scheme}://{self.host}:{self.port}"
        s = URL._socket_cache.get(cache_key)
        
        # ê¸°ì¡´ ì†Œì¼“ì´ ì—†ê±°ë‚˜ ë‹«í˜€ìˆìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if s is None:
            print(f"ğŸ”Œ ìƒˆ ì—°ê²° ìƒì„±: {cache_key}")
            s = socket.socket(
                family=socket.AF_INET,      # IPv4 ì‚¬ìš©
                type=socket.SOCK_STREAM,    # TCP ì—°ê²°
                proto=socket.IPPROTO_TCP,   # TCP í”„ë¡œí† ì½œ
            )
            
            # 2. ì„œë²„ì— ì—°ê²°
            try:
                # set a sensible timeout for network operations
                s.settimeout(10.0)
                s.connect((self.host, self.port))
            except Exception as e:
                # Ensure socket not left in cache on failure
                try:
                    s.close()
                except Exception:
                    pass
                raise Exception(f"Network error connecting to {self.host}:{self.port} - {e}")
            
            # 3. HTTPSì¸ ê²½ìš° TLSë¡œ ì•”í˜¸í™”
            if self.scheme == "https":
                ctx = ssl.create_default_context()
                s = ctx.wrap_socket(s, server_hostname=self.host)
            
            # ìºì‹œì— ì €ì¥
            URL._socket_cache[cache_key] = s
        else:
            print(f"â™»ï¸  ê¸°ì¡´ ì—°ê²° ì¬ì‚¬ìš©: {cache_key}")
        
        # 4. HTTP ìš”ì²­ ë©”ì‹œì§€ ì‘ì„± (HTTP/1.1 ì§€ì›, Keep-Alive)
        # GET ë©”ì„œë“œë¡œ íŠ¹ì • ê²½ë¡œì˜ ë¦¬ì†ŒìŠ¤ë¥¼ ìš”ì²­
        request = "GET {} HTTP/1.1\r\n".format(self.path)
        request += "Host: {}\r\n".format(self.host)
        # Keep-Alive ì‚¬ìš© (ì—°ê²° ìœ ì§€)
        request += "Connection: keep-alive\r\n"
        request += "User-Agent: Mozilla/5.0 (CustomBrowser)\r\n"
        # ì••ì¶• ì§€ì›ì„ ì„œë²„ì— ì•Œë¦¼
        request += "Accept-Encoding: gzip, deflate, br\r\n"
        request += "\r\n"  # í—¤ë”ì˜ ëì„ í‘œì‹œ
        
        # 5. ìš”ì²­ ì „ì†¡ (ë¬¸ìì—´ì„ ë°”ì´íŠ¸ë¡œ ë³€í™˜)
        s.send(request.encode("utf8"))
        
        # 6. ì‘ë‹µ ë°›ê¸° (ë°”ì´ë„ˆë¦¬ë¡œ ì½ì–´ì•¼ ì••ì¶• í•´ì œ ê°€ëŠ¥)
        try:
            response = s.makefile("rb")
        except Exception as e:
            if cache_key in URL._socket_cache:
                del URL._socket_cache[cache_key]
            s.close()
            raise Exception(f"Failed to read response from {self.host}:{self.port} - {e}")
        
        # 7. ìƒíƒœ ë¼ì¸ ì½ê¸° (ì˜ˆ: "HTTP/1.0 200 OK")
        statusline = response.readline().decode("utf8")
        version, status, explanation = statusline.split(" ", 2)
        
        # 8. ì‘ë‹µ í—¤ë” ì½ê¸° (ë¹ˆ ì¤„ì´ ë‚˜ì˜¬ ë•Œê¹Œì§€)
        response_headers = {}
        while True:
            line = response.readline().decode("utf8")
            if line == "\r\n": break  # í—¤ë”ì˜ ë
            header, value = line.split(":", 1)
            # casefold()ëŠ” lower()ë³´ë‹¤ ë” ê³µê²©ì ì¸ ëŒ€ì†Œë¬¸ì ì •ê·œí™”
            # êµ­ì œí™”ëœ ë¬¸ìë„ ì˜¬ë°”ë¥´ê²Œ ì²˜ë¦¬
            response_headers[header.casefold()] = value.strip()
        
        # 9. HTTP ìƒíƒœ í™•ì¸ ë° ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²˜ë¦¬
        status_code = int(status)
        # ë¦¬ë‹¤ì´ë ‰íŠ¸(3xx) ì²˜ë¦¬: Location í—¤ë”ê°€ ìˆìœ¼ë©´ ë”°ë¼ê°„ë‹¤
        if 300 <= status_code < 400:
            if redirects <= 0:
                raise Exception('Too many redirects')
            loc = response_headers.get('location')
            if loc:
                # ì ˆëŒ€/ìƒëŒ€ URL ëª¨ë‘ ì²˜ë¦¬
                base = f"{self.scheme}://{self.host}{self.path}"
                new_uri = urljoin(base, loc)
                # ë¡œê·¸ì— í˜„ì¬->ìƒˆ URL ê¸°ë¡
                redirect_log.append((base, new_uri))
                # ë¦¬ë‹¤ì´ë ‰íŠ¸ ì‹œ ì†Œì¼“ ìºì‹œì—ì„œ ì œê±°í•˜ê³  ë‹«ê¸°
                if cache_key in URL._socket_cache:
                    del URL._socket_cache[cache_key]
                s.close()
                # Don't return immediately â€” call inner request and then
                # let this frame finish so it can print the redirect trace
                body = URL(new_uri).request(redirects=redirects-1, redirect_log=redirect_log)
                if created_local_log and redirect_log:
                    print("Redirect trace:")
                    for i, (src, dst) in enumerate(redirect_log, 1):
                        print(f" {i}. {src} -> {dst}")
                return body
            # Locationì´ ì—†ìœ¼ë©´ ê³„ì† ì§„í–‰í•˜ì—¬ ì—ëŸ¬ ì²˜ë¦¬
        assert status_code == 200, "{}: {}".format(status_code, explanation)

        # 10. ë³¸ë¬¸(body) ì½ê¸° - Transfer-Encoding: chunked ì§€ì›
        transfer_encoding = response_headers.get("transfer-encoding", "").lower()

        def read_chunked(rfile):
            chunks = []
            trailers = {}
            while True:
                # ì²­í¬ í¬ê¸° ë¼ì¸ ì½ê¸°
                line = rfile.readline().decode("ascii")
                if not line:
                    raise Exception("Unexpected EOF while reading chunk size")
                line = line.strip()
                # ì‚¬ì´ì¦ˆ íŒŒì‹± (ì„¸ë¯¸ì½œë¡  ë’¤ì˜ ìµìŠ¤í…ì…˜ ë¬´ì‹œ)
                size_str = line.split(';', 1)[0]
                try:
                    size = int(size_str, 16)
                except ValueError:
                    raise Exception(f"Invalid chunk size: {size_str}")
                if size == 0:
                    # íŠ¸ë ˆì¼ëŸ¬ í—¤ë”(ìˆë‹¤ë©´) ì½ê¸°: ë¹ˆ ì¤„ ì „ê¹Œì§€ í—¤ë” ë¼ì¸ë“¤
                    while True:
                        trailer_line = rfile.readline().decode("utf8")
                        if trailer_line in ("\r\n", "\n", ""):
                            break
                        if ":" in trailer_line:
                            h, v = trailer_line.split(":", 1)
                            trailers[h.casefold()] = v.strip()
                    break
                data = rfile.read(size)
                chunks.append(data)
                # ì²­í¬ ëì˜ CRLF ì†Œë¹„
                rfile.read(2)
            return b"".join(chunks), trailers

        if "chunked" in transfer_encoding:
            body, trailers = read_chunked(response)
            # íŠ¸ë ˆì¼ëŸ¬ í—¤ë”ë¥¼ ì‘ë‹µ í—¤ë”ì— ë³‘í•© (ê¸°ì¡´ í—¤ë”ë¥¼ ë®ì–´ì“¸ ìˆ˜ ìˆìŒ)
            for k, v in trailers.items():
                response_headers[k] = v
        else:
            # Content-Length í—¤ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ë°”ì´íŠ¸ ìˆ˜ë§Œ ì½ê¸°
            if "content-length" in response_headers:
                length = int(response_headers["content-length"])
                body = response.read(length)
                print(f"ğŸ“¦ Content-Length: {length} ë°”ì´íŠ¸ ì½ìŒ")
            else:
                # Content-Lengthê°€ ì—†ìœ¼ë©´ ì†Œì¼“ì´ ë‹«í ë•Œê¹Œì§€ ì½ìŒ
                body = response.read()
                print("âš ï¸  Content-Length ì—†ìŒ - ì†Œì¼“ ë‹«í˜")
                # ìºì‹œì—ì„œ ì œê±°í•˜ê³  ì†Œì¼“ ë‹«ê¸°
                if cache_key in URL._socket_cache:
                    del URL._socket_cache[cache_key]
                s.close()
        
        # Connection í—¤ë” í™•ì¸í•˜ì—¬ ì†Œì¼“ ìœ ì§€ ì—¬ë¶€ ê²°ì •
        connection_header = response_headers.get("connection", "").lower()
        if "close" in connection_header:
            print("ğŸ”Œ ì„œë²„ê°€ ì—°ê²° ì¢…ë£Œ ìš”ì²­ - ì†Œì¼“ ë‹«ê¸°")
            if cache_key in URL._socket_cache:
                del URL._socket_cache[cache_key]
            s.close()
        else:
            print("âœ… ì—°ê²° ìœ ì§€ (Keep-Alive)")
        
        # 12. Content-Encodingì— ë”°ë¼ ì••ì¶• í•´ì œ
        encoding = response_headers.get("content-encoding", "").lower()
        
        if encoding == "gzip":
            print("ğŸ—œï¸  gzip ì••ì¶• í•´ì œ ì¤‘...")
            body = gzip.decompress(body)
        elif encoding == "deflate":
            print("ğŸ—œï¸  deflate ì••ì¶• í•´ì œ ì¤‘...")
            try:
                # deflateëŠ” ë‘ ê°€ì§€ í˜•ì‹ì´ ìˆìŒ (zlib í—¤ë” ìˆìŒ/ì—†ìŒ)
                body = zlib.decompress(body)
            except zlib.error:
                # zlib í—¤ë”ê°€ ì—†ëŠ” ê²½ìš° raw deflate ì‹œë„
                body = zlib.decompress(body, -zlib.MAX_WBITS)
        elif encoding == "br":
                body = brotli.decompress(body)
        elif encoding:
            # ì•Œ ìˆ˜ ì—†ëŠ” ì¸ì½”ë”©
            raise Exception(f"Unsupported content-encoding: {encoding}")
        else:
            print("ğŸ“„ ì••ì¶• ì—†ìŒ")
        
        # 13. ë°”ì´íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        body = body.decode("utf8", errors="replace")
        
        # 14. ìºì‹œ ì €ì¥ (200 OK ì‘ë‹µì´ê³  ìºì‹œ ê°€ëŠ¥í•œ ë¦¬ì†ŒìŠ¤ì¸ ê²½ìš°)
        if status_code == 200 and is_cacheable:
            cache_control = response_headers.get('cache-control', '')
            no_store, max_age = URL._parse_cache_control(cache_control)
            
            if not no_store:
                # no-storeê°€ ì•„ë‹ˆë©´ ìºì‹œì— ì €ì¥
                URL._save_to_cache(full_url, body, response_headers, max_age)
            else:
                print(f"ğŸš« ìºì‹œ ê¸ˆì§€ (no-store): {full_url}")

        # If we created the redirect_log in this call and there are entries,
        # print the redirect trace for non-view-source requests as well.
        if created_local_log and redirect_log:
            print("Redirect trace:")
            for i, (src, dst) in enumerate(redirect_log, 1):
                print(f" {i}. {src} -> {dst}")

        return body


# Ensure sockets in the socket cache are closed on program exit
def _close_socket_cache():
    for key, s in list(URL._socket_cache.items()):
        try:
            s.close()
        except Exception:
            pass
    URL._socket_cache.clear()

atexit.register(_close_socket_cache)



